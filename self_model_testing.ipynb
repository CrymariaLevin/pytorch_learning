{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "4.12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertConfig, BertForQuestionAnswering, BertModel\n",
    "from transformers import InputExample\n",
    "from transformers.data.processors.squad import SquadV2Processor, squad_convert_examples_to_features\n",
    "from transformers.data.datasets.squad import SquadDataset\n",
    "\n",
    "config = BertConfig.from_pretrained('../NLP_models/bert-base-chinese')\n",
    "# config = BertConfig.from_pretrained('chinese-bert-wwm')\n",
    "print(config.hidden_size)\n",
    "# config.to_json_file(\"self_QA/config.json\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../NLP_models/bert-base-chinese\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, mode='train'):\n",
    "        \n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)[\"data\"]\n",
    "        self.samples = []\n",
    "        self.long_samples = []  # 记录输入文本长度大于512的样本的索引\n",
    "        for item in self.data:\n",
    "            for paragraph in item[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    question = qa[\"question\"]\n",
    "                    id = qa[\"id\"]\n",
    "                    if mode == 'train':\n",
    "                        answer_start = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "                        answer_text = [answer[\"text\"] for answer in qa[\"answers\"]]\n",
    "                    else: # 验证集会有多个答案(不用这样也可以，不传入mode参数即可)\n",
    "                        answer_start = [max(answer[\"answer_start\"] for answer in qa[\"answers\"])]\n",
    "                        answer_text = [max(answer[\"text\"] for answer in qa[\"answers\"])]\n",
    "                    if len(answer_start) == 0 or len(answer_text) == 0: # 如果答案为空，忽略该样本\n",
    "                        continue\n",
    "#                     input_dict = tokenizer.encode_plus(\n",
    "#                         question, context, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "#                     )\n",
    "#                     if input_dict[\"input_ids\"].shape[1] > 512:  # 如果输入文本长度大于512，记录索引\n",
    "#                         self.long_samples.append(len(self.samples))\n",
    "                    self.samples.append({\n",
    "                        \"id\": id,\n",
    "                        \"context\": context,\n",
    "                        \"question\": question,\n",
    "                        \"answer_start\": answer_start,\n",
    "                        \"answer_text\": answer_text\n",
    "                    })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        # # 这里通过Tokenizer的encode_plus方法将question和context拼接并转化为input_ids, attention_mask, token_type_ids这三个参数\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            sample[\"question\"], sample[\"context\"], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        start_positions = torch.tensor(sample[\"answer_start\"], dtype=torch.long)\n",
    "        end_positions = torch.tensor([start_position + len(answer_text) - 1 for start_position, answer_text in zip(sample[\"answer_start\"], sample[\"answer_text\"])], dtype=torch.long)\n",
    "\n",
    "        # 截断设置，可能不太合理\n",
    "        start_positions[(start_positions >= 512) | (start_positions < 0)] = 510\n",
    "        end_positions[(end_positions >= 512) | (end_positions < 0)] = 511        \n",
    "        \n",
    "        input_dict[\"start_positions\"] = start_positions\n",
    "        input_dict[\"end_positions\"] = end_positions\n",
    "        \n",
    "        return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e139eef6-fc0c-4953-acec-a83a0095ce4e.txt_003',\n",
       " 'context': '经审查,原告提供的证据1-3、被告中华联合广东分公司提供的证据4-5、被告万友公司提供的证据6,各方对其真实性均没有异议,本院对其真实性予以确认综合本院采信的证据及当事人的陈述,本院认定以下事实:2015年6月1日,田x17驾驶粤A×××××号车辆与严x3驾驶的赣C×××××号重型仓栅式货车发生碰撞,造成两车不同程度损坏的交通事故交警部门作出事故认定书,认定严x3承担事故的全部责任,田x17不负事故责任粤A×××××号车辆在原告处投保了保险金额为908000元的机动车损失保险,事故发生在保险期间内事故发生后,粤A×××××号车辆的被保险人陈x18就该车辆的损失以财产保险合同纠纷起诉至佛山市禅城区人民法院案经审理,佛山市禅城区人民法院于2015年8月18日作出(2015)佛城法民二初字第1006号民事判决,查明粤A×××××号车辆经广州市华盟价格事务所有限公司评估,损失价格为241541元,陈x18支付了粤A×××××号车辆的维修费241541元、评估费9050元;本案原告在庭审中明确表示不申请重新对车辆损失进行评估鉴定并判决原告向陈x18支付粤A×××××号车辆损失保险理赔款250591元2015年10月11日,原告向陈x18赔付了250591元及诉讼费用2529元后原告提起本案之诉并查明,赣C×××××号车辆的所有人为被告万友公司,该车辆在被告中华联合广东分公司处投保了交强险,事故发生在保险期内事故发生后,被告中华联合广东分公司向该车辆的被保险人许x19赔付了2000元诉讼中,被告徐11确认其为该车辆的实际支配人,严x3是被告徐11雇请,是从事派遣工作过程中发生案涉交通事故被告徐11与被告万友公司签订《车辆挂靠合同书》,被告万友公司同意被告徐11就赣C×××××号车辆挂靠被告万友公司名下',\n",
       " 'question': '投保人所投保险险种？',\n",
       " 'answer_start': [233],\n",
       " 'answer_text': ['机动车损失保险']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path =\"./CJRC/transfered/big_train_data.json\"\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "samples = []\n",
    "for item in data:\n",
    "    for paragraph in item[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            id = qa[\"id\"]\n",
    "            answer_start = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n",
    "            answer_text = [answer[\"text\"] for answer in qa[\"answers\"]]\n",
    "            samples.append({\n",
    "                \"id\": id,\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "                \"answer_start\": answer_start,\n",
    "                \"answer_text\": answer_text\n",
    "            })\n",
    "#     break\n",
    "\n",
    "samples[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(samples[2]['answer_start'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [ast['answer_start'] for ast in samples if len(ast['answer_start']) > 1]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[ 101,  752, 3125,  ..., 5468, 1394,  102]],\n",
       "\n",
       "        [[ 101,  752, 3125,  ..., 6158, 1440,  102]],\n",
       "\n",
       "        [[ 101, 2832,  924,  ..., 1440,  704,  102]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 101, 6158, 1440,  ...,    0,    0,    0]],\n",
       "\n",
       "        [[ 101, 1333, 6158,  ...,    0,    0,    0]],\n",
       "\n",
       "        [[ 101, 1352, 3175,  ...,    0,    0,    0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 1, 1, 1]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'start_positions': tensor([[153],\n",
       "        [180],\n",
       "        [233],\n",
       "        [225],\n",
       "        [ 30],\n",
       "        [ 54],\n",
       "        [180],\n",
       "        [510],\n",
       "        [120],\n",
       "        [ 33],\n",
       "        [114],\n",
       "        [ 68],\n",
       "        [ 79],\n",
       "        [ 11],\n",
       "        [ 11],\n",
       "        [  8]]), 'end_positions': tensor([[160],\n",
       "        [202],\n",
       "        [239],\n",
       "        [231],\n",
       "        [ 33],\n",
       "        [179],\n",
       "        [189],\n",
       "        [511],\n",
       "        [126],\n",
       "        [ 50],\n",
       "        [121],\n",
       "        [ 78],\n",
       "        [ 96],\n",
       "        [ 25],\n",
       "        [ 20],\n",
       "        [  9]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path =\"./CJRC/transfered/big_train_data.json\"\n",
    "\n",
    "train_dataset = MyDataset(path)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=0 # 这个是多线程数，最好设为0\n",
    ")\n",
    "\n",
    "next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertQA(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_path, config_path):\n",
    "        super(BertQA, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(config_path)\n",
    "        self.bert = BertModel.from_pretrained(pretrained_path, config=self.config)\n",
    "        self.dropout = torch.nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.qa_outputs = torch.nn.Linear(self.bert.config.hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "依据上述有效证据,本院确认以下案件事实:原、被告双方于××××年××月份结婚,婚后生一女孩,取名马62,2013年11月21日,双方在新民市民政局协议离婚,协议内容为:婚生女孩马62由女方别x2抚养,男方每月给孩子抚养费1000元(从2013年12月1日起至2020年12月止)双方离婚后,小孩马62一直随被告生活××××年××月××日,婚生女孩马62从内乡县第二小学转入沈阳大学新民师范学院附属小学上学至今,现原告以小孩马62已年满十周岁,自愿提出与原告一起生活为由,诉至法院,请求依法变更小孩马62由原告抚养。\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "context = \"依据上述有效证据,本院确认以下案件事实:原、被告双方于××××年××月份结婚,婚后生一女孩,取名马62,2013年11月21日,\\\n",
    "双方在新民市民政局协议离婚,协议内容为:婚生女孩马62由女方别x2抚养,男方每月给孩子抚养费1000元(从2013年12月1日起至2020年12月止)\\\n",
    "双方离婚后,小孩马62一直随被告生活××××年××月××日,婚生女孩马62从内乡县第二小学转入沈阳大学新民师范学院附属小学上学至今,\\\n",
    "现原告以小孩马62已年满十周岁,自愿提出与原告一起生活为由,诉至法院,请求依法变更小孩马62由原告抚养。\"\n",
    "question = \"离婚协议约定马62由谁抚养？\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../NLP_models/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertQA('../NLP_models/bert-base-chinese',config_path=\"../NLP_models/bert-base-chinese/config.json\")\n",
    "checkpoint = torch.load('../NLP_models/self_models/cjrc_model_dict.pth.bar',map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3581f919c12d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# print(start_logits, end_logits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_logits\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "\n",
    "question = \"谁胜诉了？\"\n",
    "context = '''一、被告 东营美利达新型材料科技有限公司 于本判决生效之日起十日内支付原告 山东富润润滑油股份有限公司 货款153540元、违约金23584.5及自2021年3月20日起至清偿之日止的违约金（违约金计算方式：以153540元为基数，按日万分之五计算）；\\r\\n二、驳回原告<a href=\"https://www.tianyancha.com/company/3366658716\" target=\"_blank\" data-type=\"company\">山东富润润滑油股份有限公司</a>的其他诉讼请求。\\r\\n如果未按本判决指定的期间履行给付金钱义务，应当依照《中华人民共和国民事诉讼法》第二百六十条规定，加倍支付迟延履行期间的债务利息。\\r\\n案件受理费5357元，减半收取计2678.5元，由被告<a href=\"https://www.tianyancha.com/company/2339951049\" target=\"_blank\" data-type=\"company\">东营美利达新型材料科技有限公司</a>负担，于本判决生效后七日内向本院交纳。\\r\\n如不服本判决，可以在判决书送达之日起十五日内，向本院递交上诉状，并按对方当事人的人数提出副本，同时按照不服本判决部分的上诉请求数额，交纳案件受理费，上诉于山东省东营市中级人民法院。上诉期满后七日内仍未交纳上诉案件受理费的，按自动撤回上诉处理。'''\n",
    "\n",
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "# 获取答案开始和结束位置\n",
    "start_logits, end_logits = model(**inputs)\n",
    "# print(start_logits, end_logits)\n",
    "\n",
    "start = torch.argmax(start_logits)\n",
    "end = torch.argmax(end_logits) + 1\n",
    "print(start,end)\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end]))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"北京大学成立于何时？\"\n",
    "context = \"北京大学创立于1898年，前身为京师大学堂，是中国近现代第一所国立综合性大学...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../NLP_models/bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../NLP_models/bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model answer question: \n",
      " {'score': 0.002553990576416254, 'start': 0, 'end': 40, 'answer': '北京大学创立于1898年，前身为京师大学堂，是中国近现代第一所国立综合性大学...'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"../NLP_models/bert-base-chinese\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(\"model answer question: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../NLP_models/bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../NLP_models/bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38) tensor(1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "\n",
    "# 加载已训练好的模型\n",
    "model_path = \"../NLP_models/bert-base-chinese\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "# 加载tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../NLP_models/bert-base-chinese\")\n",
    "\n",
    "# 设置测试用例\n",
    "question = \"北京大学成立于何时？\"\n",
    "context = \"北京大学创立于1898年，前身为京师大学堂，是中国近现代第一所国立综合性大学...\"\n",
    "\n",
    "# 对测试用例进行编码\n",
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "# 获取答案开始和结束位置\n",
    "outputs = model(**inputs)\n",
    "start = torch.argmax(outputs.start_logits)\n",
    "end = torch.argmax(outputs.end_logits) + 1\n",
    "print(start,end)\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start:end]))\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
