{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Bert模型和自定义数据训练chat模型\n",
    "\n",
    "自定义数据需将数据转换为需要的格式。对于Bert模型，需要将数据转换为BERT的输入格式，即token和segment embeddings的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class BertForQuestionAnswering(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "#         self.bert = BertModel.from_pretrained('bert-base-chinese', config=config)        \n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits\n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "class QAData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __getitem__(self, index):\n",
    "        example = self.data[index]\n",
    "        question = example['question']\n",
    "        answer = example['answer']\n",
    "        inputs = tokenizer(question, answer, return_tensors='pt', padding=True, truncation=True)\n",
    "        start_positions = torch.tensor([example['start_position']])\n",
    "        end_positions = torch.tensor([example['end_position']])\n",
    "        return inputs, start_positions, end_positions\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "# config = BertConfig.from_pretrained('bert-base-chinese')\n",
    "\n",
    "model = BertForQuestionAnswering(config)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_data = [...] # 自定义训练数据\n",
    "\n",
    "train_dataset = QAData(train_data)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, start_positions, end_positions = batch\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs[0], start_positions) + loss_fn(outputs[1], end_positions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "test_data = [...] # 自定义测试数据\n",
    "test_dataset = QAData(test_data)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs, start_positions, end_positions = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_preds = torch.argmax(outputs[0], dim=1)\n",
    "    end_preds = torch.argmax(outputs[1], dim=1)\n",
    "    # 计算准确率等指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义数据train_data通常包括一系列问题和对应的答案，每个问题和答案都需要对应的起始位置和结束位置，用于训练模型预测答案的位置。下面是一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    {\n",
    "        'question': 'What is the capital of China?',\n",
    "        'answer': 'Beijing is the capital of China.',\n",
    "        'start_position': 23,\n",
    "        'end_position': 30\n",
    "    },\n",
    "    {\n",
    "        'question': 'Who is the author of The Great Gatsby?',\n",
    "        'answer': 'The author of The Great Gatsby is F. Scott Fitzgerald.',\n",
    "        'start_position': 27,\n",
    "        'end_position': 46\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the highest mountain in the world?',\n",
    "        'answer': 'Mount Everest is the highest mountain in the world.',\n",
    "        'start_position': 0,\n",
    "        'end_position': 12\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用中文的SQuAD数据集进行训练，可以使用Hugging Face的transformers库中的SquadDataset类来加载数据集。同时，为了添加自定义的训练数据，需要将自定义数据加入到SQuAD数据集中。下面是修改后的代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import InputExample\n",
    "from transformers.data.processors.squad import SquadV2Processor, squad_convert_examples_to_features\n",
    "from transformers.data.datasets.squad import SquadDataset\n",
    "\n",
    "\n",
    "class BertForQuestionAnswering(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnswering, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-chinese', config=config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "processor = SquadV2Processor()\n",
    "train_examples = processor.get_train_examples('data/', filename='train-v2.0.json')\n",
    "\n",
    "custom_train_data = [\n",
    "    {\n",
    "        'question': '什么是Python?',\n",
    "        'answer': 'Python是一种解释型、高级的、通用的编程语言。',\n",
    "        'start_position': 0,\n",
    "        'end_position': 6\n",
    "    },\n",
    "    {\n",
    "        'question': '什么是深度学习?',\n",
    "        'answer': '深度学习是一种机器学习技术，它允许计算机模拟人类大脑进行学习和理解。',\n",
    "        'start_position': 0,\n",
    "        'end_position': 4\n",
    "    }\n",
    "]\n",
    "\n",
    "custom_train_examples = [\n",
    "    InputExample(qa_id=str(i), question=example['question'], context='', answer_text=example['answer'], start_position_answer=example['start_position'], end_position_answer=example['end_position']) for i, example in enumerate(custom_train_data)\n",
    "]\n",
    "\n",
    "train_examples += custom_train_examples\n",
    "train_features, _ = squad_convert_examples_to_features(train_examples, tokenizer, max_seq_length=512, doc_stride=128, max_query_length=64, is_training=True)\n",
    "train_dataset = SquadDataset(train_features)\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-chinese')\n",
    "\n",
    "model = BertForQuestionAnswering(config)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'token_type_ids': batch[2],\n",
    "        }\n",
    "        start_positions = batch[3]\n",
    "        end_positions = batch[4]\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs[0], start_positions) + loss_fn(outputs[1], end_positions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "test_examples = processor.get_dev_examples('data/', filename='dev-v2.0.json')\n",
    "test_features, _ = squad_convert_examples_to_features(test_examples, tokenizer, max_seq_length=512, doc_stride=128, max_query_length=64, is_training=False)\n",
    "test_dataset = SquadDataset(test_features)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs = {\n",
    "        'input_ids': batch[0],\n",
    "        'attention_mask': batch[1],\n",
    "        'token_type_ids': batch[2],\n",
    "    }\n",
    "    start_positions = batch[3]\n",
    "    end_positions = batch[4]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    start_preds = torch.argmax(outputs[0], dim=1)\n",
    "    end_preds = torch.argmax(outputs[1], dim=1)\n",
    "    # 计算准确率等指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个示例中，我们首先使用SquadV2Processor从文件中加载中文SQuAD数据集中的训练数据，并使用SquadDataset类将其转换为PyTorch的Dataset对象。同时，我们将自定义的训练数据加入到SQuAD数据集中，并将其转换为InputExample格式的对象，再调用squad_convert_examples_to_features函数将其转换为模型需要的特征格式。在训练和测试阶段，我们使用PyTorch的DataLoader将数据分批加载，并按照BERT模型的输入格式将数据传入模型中进行训练和预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
